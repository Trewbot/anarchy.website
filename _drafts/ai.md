---
author: uja
layout: post
license: BY-SA
tags: >
    philosophy science ai artificial-intelligence transhumanism technology rant
    future
title: Humanity's Extinction at the Will of Artificial Intelligence
---

According to the [Future of Humanity Institute][1]'s 2008 technical report
titled ["Global Catastrophic Risk Survey"][2] by Anders Sandberg and Nick
Bostrom there is a 5% chance that a superintelligent artificial intelligence
(AI) will cause the extinction of humanity by the year 2100, and I'm here to
tell you why that is fucking stupid. I considered talking about this a few
months ago after seeing the estimates from this study [on Wikipedia][3]:

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">
Also, I could rant about this for days but no AI would kill off humanity.</p>
&mdash; una (@trewbot)
<a href="https://twitter.com/trewbot/status/969113718603870208">March 1,2018</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8">
</script>

But since the big, scary rise of superintelligent AI isn't around the corner and
2100 is probably never even going to happen, I just sort of put this in the back
of my mind with all the other stupid things people say at or around me. That was
until I, as the obsessive egotist I am, was looking back at some old tweets of
mine and saw the one embedded above once more.

To begin with, as with all discussions, I ought to clarify some definitions to
ensure we're all on the same page and I don't have to spend weeks listening to
people tell me about how I'm wrong solely because they use a different word for
the thing I said. Artificial intelligence here refers to any system created
either directly or indirectly by humans that uses analysis of its environment
(both in terms of information and physical reality) to determine the actions it
needs to take towards a specific goal; yes, this is very heavily inspired by the
[Wikipedia definition][4], I already had a basic idea for it but I needed
something a bit more concrete to use here.

Typical modern AI requires some level of human involvement in the learning
process, be it through manual coding of decisions or through written sets of
input and expected output. In theory, the next major step forward in AI would be
the removal of this necessity, general AI that can adapt to environments other
than the extremely sterile ones presented to them by their creators. Of course,
the concerns presented by the aforementioned report are about something called
"superintelligent AI" which takes this even further: intelligence beyond that of
humans. For a more thorough definition, I'll just use Nick Bostrom's definition:

> Any intellect that greatly exceeds the cognitive performance of humans in
virtually all domains of interest.

Let's not keep track of how many times I have to drop that name now, otherwise
this will start looking like a totally different type of post.

<div class="pullquote image left">
    <img src="https://www.fhi.ox.ac.uk/wp-content/uploads/DSC_0074.jpg">
    Nick Bostrom,
    <i>source: <a href="https://www.fhi.ox.ac.uk/press/images/">Future of
        Humanity Institute</a></i>
</div>

In case you couldn't tell from my tone or the words I'm saying, I'm fairly
biased here, so let's talk about biases. I'm a 20-year-old living in the Midwest
that failed out of college (twice) as a Physics and Mathematics double major
with a minor (formerly first major) in Computer Science. I sleep on the floor
and get into Twitter arguments about anarchism which is the ideal organization
for human society. Nick Bostrom, our primary opposition here, has a PhD in
philosophy from the London School of Economics. He's credited with the idea of
existential risk and founded the Future of Humanity Institute at the University
of Oxford in 2005.

<div class="pullquote image">
    <img src="/assets/img/me.jpg">
    Me,
    <i>source: <a href="/">anarchy.website</a></i>
</div>

That all said, it sounds more like I'm a total dumbass and this guy has really
put a lot of thought into this. Of course, that is my exact point here: he seems
to have dedicated quite a bit of his life to this. Obviously this isn't the only
idea he's concerned himself with; my first time finding out about him was
actually through the concept of ancestor simulations and the simulation
hypothesis (something Elon Musk is hella into).

Now, I'm not the first person to hear about this whole thing and immediately
think it's some fearmongering bullshit. In 2017, [Daniel Jeffries][5] wrote a
piece on [Medium][6] called [Why Superintelligent AI Will Kick Ass][7]. Hard to
tell from that title what their position is; that's a joke, though I'm relying
heavily on the title here because the first few paragraphs had way too much
pro-capitalist bullshit to rationalize continuing to read. If we go back a bit
further we'll see a much nicer\* article from 2016 by Oren Etzioni in the
[MIT Technology Review][8] called [No, the Experts Don’t Think Superintelligent
AI is a Threat to Humanity][9].

\* "Much nicer" here is more aptly "slightly less shit." This article is
primarily concerned with the survey methodology Bostrom used in his book
[_Superintelligence: Paths, Dangers, and Strategies_][10]. I'd prefer a more
direct method of actually reading the book myself but I don't have the money for
that so this'll have to do (this isn't meant to really be a direct refutation of
Bostrom anyway, lol). Furthermore, the article is making a point about the
likelihood of us having superintelligent AI within the next 25 to 100 years.
While this is perhaps a relevant discussion here, I'm just going to bypass this
all and make the worst (or best?) case assumption that we will have
superintelligent AI in the near future.

[Allan Dafoe][11] (who is also associated with the FHI) and [Stuart Russell][12]
wrote a brief critique of Etzioni's article fittingly called [Yes, We Are
Worried About the Existential Risk of Artificial Intelligence][13], which was
also published in the MIT Technology Review in late 2016. Thankfully, this one
re-centers the conversation less on the prediction of when we will have to deal
with superintelligent AI and more on what dealing with them will look like:

>It’s important to understand that Etzioni is not even addressing the reason
*Superintelligence* has had the impact he decries: its clear explanation of why
superintelligent AI may have arbitrarily negative consequences and why it’s
important to begin addressing the issue well in advance. Bostrom *does not* base
his case on predictions that superhuman AI systems are imminent. He writes, “It
is no part of the argument in this book that we are on the threshold of a big
breakthrough in artificial intelligence, or that we can predict with any
precision when such a development might occur.”

<div class="pullquote">
    Clearly this is a bit of a hot topic in the AI community.
</div>

Sadly that's about as far as they go with that, but it's forgivable since this
is very definitely just a response to Etzioni and nothing else. Clearly this is
a bit of a hot topic in the AI community. Shocking, I know. In my outline for
this post I just wrote "biases" here and I think I've sufficiently expanded on
that, so let's just move on to "philosophical arguments" already.

[1]: https://www.fhi.ox.ac.uk/
[2]: https://www.fhi.ox.ac.uk/reports/2008-1.pdf
[3]: https://en.wikipedia.org/wiki/Global_catastrophic_risk
[4]: https://en.wikipedia.org/wiki/Artificial_intelligence
[5]: https://hackernoon.com/@dan.jeffries
[6]: https://medium.com/
[7]: https://hackernoon.com/why-superintelligent-ai-will-kick-ass-38f8b25978c0
[8]: https://www.technologyreview.com/
[9]: https://www.technologyreview.com/s/602410/no-the-experts-dont-think-superintelligent-ai-is-a-threat-to-humanity/
[10]: https://global.oup.com/academic/product/superintelligence-9780199678112?cc=us&lang=en&
[11]: http://www.allandafoe.com/
[12]: http://people.eecs.berkeley.edu/~russell/
[13]: https://www.technologyreview.com/s/602776/yes-we-are-worried-about-the-existential-risk-of-artificial-intelligence/
