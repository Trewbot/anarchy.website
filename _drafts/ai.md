---
author: uja
layout: post
tags: philosophy science ai transhumanism
title: Humanity's Extinction at the Will of Artificial Intelligence
---

According to the [Future of Humanity Institute][1]'s 2008 technical report
titled ["Global Catastrophic Risk Survey"][2] by Anders Sandberg and Nick
Bostrom there is a 5% chance that a superintelligent artificial intelligence
(AI) will cause the extinction of humanity by the year 2100, and I'm here to
tell you why that is fucking stupid. I considered talking about this a few
months ago after seeing the estimates from this study [on Wikipedia][3]:

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">
Also, I could rant about this for days but no AI would kill off humanity.</p>
&mdash; una (@trewbot)
<a href="https://twitter.com/trewbot/status/969113718603870208">March 1,2018</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8">
</script>

But since the big, scary rise of superintelligent AI isn't around the corner and
2100 is probably never even going to happen, I just sort of put this in the back
of my mind with all the other stupid things people say at or around me. That was
until I, as the obsessive egotist I am, was looking back at some old tweets of
mine and saw the one embedded above once more.

- defintions

- biases

- philosophical arguments

- conclusion/summary

[1]: https://www.fhi.ox.ac.uk/
[2]: https://www.fhi.ox.ac.uk/reports/2008-1.pdf
[3]: https://en.wikipedia.org/wiki/Global_catastrophic_risk
